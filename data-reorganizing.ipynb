{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f67c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f287c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_features(file_path, sheet_name=2):\n",
    "    \"\"\"Extracts RMS from high-frequency sensor data.\"\"\"\n",
    "    try:\n",
    "        df_signal = pd.read_csv(file_path, sheet_name=sheet_name)\n",
    "        # We focus on the 2000Hz column as it has the most data density\n",
    "        # Handle NaNs by dropping them for the math\n",
    "        signal = df_signal['F1_2000Hz'].dropna().values\n",
    "        if len(signal) == 0: return 0\n",
    "        \n",
    "        # Root Mean Square (RMS) calculation\n",
    "        rms = np.sqrt(np.mean(signal**2))\n",
    "        return rms\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fdcebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(main_data_path, sensor_folder):\n",
    "    # Load your main clinical file (Biochem, Demographics, PA)\n",
    "    # Assuming this is a merged file where each row is a unique ID\n",
    "    df_clinical = pd.read_excel(main_data_path) # or pd.read_csv\n",
    "    \n",
    "    # Placeholder for sensor features\n",
    "    sensor_features = []\n",
    "\n",
    "    print(f\"Processing {len(df_clinical)} patients...\")\n",
    "\n",
    "    for index, row in df_clinical.iterrows():\n",
    "        p_id = str(int(row['ID'])).zfill(4) # Formats ID to 0001, 0002 etc\n",
    "        \n",
    "        # Find all test files for this specific patient\n",
    "        search_pattern = os.path.join(sensor_folder, f\"in_test_{p_id}_*.csv\")\n",
    "        patient_files = glob.glob(search_pattern)\n",
    "        \n",
    "        if patient_files:\n",
    "            # Calculate RMS for every file (rep/angle) found for this patient\n",
    "            rms_values = [extract_signal_features(f) for f in patient_files]\n",
    "            avg_rms = np.mean(rms_values)\n",
    "        else:\n",
    "            avg_rms = np.nan # No sensor data found\n",
    "            \n",
    "        sensor_features.append(avg_rms)\n",
    "\n",
    "    df_clinical['Sensor_RMS_Intensity'] = sensor_features\n",
    "    return df_clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82d6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metabolic_label(df):\n",
    "    \"\"\"\n",
    "    Creates a 'Disease' label based on IDF criteria.\n",
    "    Adjust column names to match your file exactly.\n",
    "    \"\"\"\n",
    "    def logic(row):\n",
    "        score = 0\n",
    "        # 1. Fasting Glucose > 100\n",
    "        if row.get('Glucose', 0) > 100: score += 1\n",
    "        # 2. Triglycerides > 150\n",
    "        if row.get('Triglycerides', 0) > 150: score += 1\n",
    "        # 3. BMI > 30 (as proxy for Waist Circumference)\n",
    "        if row.get('BC_BMI', 0) > 30: score += 1\n",
    "        # 4. Blood Pressure Systolic > 130\n",
    "        if row.get('BP_Systolic', 0) > 130: score += 1\n",
    "        \n",
    "        return 1 if score >= 2 else 0 # 2+ markers indicates risk\n",
    "\n",
    "    df['Target_Metabolic_Disease'] = df.apply(logic, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0afd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_biochem_demographics.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m clinical_path = \u001b[33m\"\u001b[39m\u001b[33mpath_to_your_biochem_demographics.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m sensor_dir = \u001b[33m\"\u001b[39m\u001b[33mfolder_with_2000Hz_csvs/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = \u001b[43mload_and_merge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclinical_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m data = create_metabolic_label(data)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Drop rows where we are missing critical predictors or the label\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_and_merge_data\u001b[39m\u001b[34m(main_data_path, sensor_folder)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_merge_data\u001b[39m(main_data_path, sensor_folder):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Load your main clinical file (Biochem, Demographics, PA)\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Assuming this is a merged file where each row is a unique ID\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     df_clinical = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_data_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# or pd.read_csv\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Placeholder for sensor features\u001b[39;00m\n\u001b[32m      7\u001b[39m     sensor_features = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ameyd\\.conda\\envs\\epf\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ameyd\\.conda\\envs\\epf\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ameyd\\.conda\\envs\\epf\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ameyd\\.conda\\envs\\epf\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'path_to_your_biochem_demographics.xlsx'"
     ]
    }
   ],
   "source": [
    "sex = \"meta/DB_QUE_Overall_Health_Tempus_2025_09_17_Toni.xlsx\"\n",
    "sensor_dir = \"C:\\\\DumbStuff\\\\epf study\\\\Meta-Elasto\\\\els\\\\meta\\\\Elastography_rawdata\\\\oldcode\\\\\"\n",
    "age = \"meta/DB_RP_TEMPUS_2025_10_09_Toni.xlsx\"\n",
    "data = load_and_merge_data(sex, sensor_dir)\n",
    "data = create_metabolic_label(data)\n",
    "\n",
    "# Drop rows where we are missing critical predictors or the label\n",
    "data = data.dropna(subset=['Target_Metabolic_Disease', 'Sensor_RMS_Intensity'])\n",
    "\n",
    "# Select potential features (Add your column names here)\n",
    "features = [\n",
    "    'Age', 'OH_DEMO_sex', 'BC_BMI', 'dur_day_total_MVPA_bts_5_min_wei', \n",
    "    'Sensor_RMS_Intensity', 'HDL_Cholesterol', 'Some_Reproductive_Marker'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['Target_Metabolic_Disease']\n",
    "\n",
    "# Standardize data (Critical for small datasets)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3560f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector_model.fit(X_scaled, y)\n",
    "\n",
    "importances = pd.Series(selector_model.feature_importances_, index=features)\n",
    "print(\"\\n--- Feature Importance ---\")\n",
    "print(importances.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform Leave-One-Out Cross Validation\n",
    "scores = cross_val_score(model, X_scaled, y, cv=loo)\n",
    "\n",
    "print(\"\\n--- Model Results ---\")\n",
    "print(f\"Mean Accuracy: {scores.mean() * 100:.2f}%\")\n",
    "print(f\"Standard Deviation: {scores.std():.2f}\")\n",
    "\n",
    "# Final training on whole set to see classification report\n",
    "model.fit(X_scaled, y)\n",
    "y_pred = model.predict(X_scaled)\n",
    "print(\"\\nFinal Classification Report (On Full Training Set):\")\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba07e4",
   "metadata": {},
   "source": [
    "# Damn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ccb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_health = \"meta/DB_QUE_Overall_Health_Tempus_2025_09_17_Toni.xlsx\"\n",
    "path_rp     = \"meta/DB_RP_TEMPUS_2025_10_09_Toni.xlsx\"\n",
    "path_pa     = \"DB_PA_TEMPUS_Short_Version_Toni.csv\"\n",
    "sensor_dir  = \"C:\\\\DumbStuff\\\\epf study\\\\Meta-Elasto\\\\els\\\\meta\\\\Elastography_rawdata\\\\oldcode\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009c7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_master_data():\n",
    "    # Load separate files\n",
    "    df_health = pd.read_excel(path_health)\n",
    "    df_rp     = pd.read_excel(path_rp)\n",
    "    df_pa     = pd.read_csv(path_pa)\n",
    "    \n",
    "    # IMPORTANT: We focus only on measurement 't1' (Baseline) for diagnosis\n",
    "    df_pa = df_pa[df_pa['measurement'] == 't1']\n",
    "    \n",
    "    # Merge Step 1: Health + Reproductive Profile\n",
    "    # Assuming both use 'ID' as the common column\n",
    "    combined = pd.merge(df_health, df_rp, on='ID', how='inner')\n",
    "    \n",
    "    # Merge Step 2: Add Physical Activity\n",
    "    combined = pd.merge(combined, df_pa, on='ID', how='inner')\n",
    "    \n",
    "    # Merge Step 3: Add Biochemistry (Replace with your actual biochem path if separate)\n",
    "    # combined = pd.merge(combined, df_biochem, on='ID', how='inner')\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443afec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3145ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: meta\\DB_BC_Tempus_2025_09_17_Toni.xlsx\n",
      "Processing file: meta\\DB_BLOB_Tempus_2025_09_17_Toni.xlsx\n",
      "Processing file: meta\\DB_MEDication_Tempus_2025_09_15_Toni.xlsx\n",
      "Processing file: meta\\DB_NUT_R24H_Tempus_2025_01_10_Toni.xlsx\n",
      "Processing file: meta\\DB_QUE_Overall_Health_Tempus_2025_09_17_Toni.xlsx\n",
      "Processing file: meta\\DB_RP_TEMPUS_2025_10_09_Toni.xlsx\n",
      "Processing file: meta\\measured_with_elastograph_patients.xlsx\n",
      "Processing file: meta\\patients_newcode_physics.xlsx\n",
      "Processing file: meta\\patients_oldcode_physics.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder = \"meta\"\n",
    "pattern = os.path.join(folder, \"*.xlsx\")\n",
    "files = glob.glob(pattern)  # all Excel files in folder [web:6][web:9]\n",
    "\n",
    "target_cols = [\"BC_WC_mean_PRE\", \"BC_WC_mean_POST\", \"BC_WC_mean_FUP\"]  # columns you want\n",
    "\n",
    "# # Option 1: store per-file extracted data\n",
    "# extracted_per_file = {}\n",
    "\n",
    "# for f in files:\n",
    "#     df = pd.read_excel(f)  # read whole file [web:6]\n",
    "#     present = [c for c in target_cols if c in df.columns]\n",
    "#     if present:\n",
    "#         extracted_per_file[os.path.basename(f)] = df[present].copy()\n",
    "\n",
    "# Option 2: combine all found columns into one DataFrame\n",
    "combined_list = []\n",
    "for f in files:\n",
    "    print(f\"Processing file: {f}\")\n",
    "    df = pd.read_excel(f)\n",
    "    present = [c for c in target_cols if c in df.columns]\n",
    "    if present:\n",
    "        tmp = df[present].copy()\n",
    "        tmp[\"source_file\"] = os.path.basename(f)\n",
    "        combined_list.append(tmp)\n",
    "        print(f\"Extracted from {f}: columns {present}\")\n",
    "\n",
    "combined = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf88c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('meta/DB_BC_Tempus_2025_09_17_Toni.xlsx', sheet_name='PRE')  # replace with your file path [web:11]\n",
    "col1 = df['BC_WC_mean_PRE'].tolist()\n",
    "col2 = df['ID'].tolist()\n",
    "col3 = df['Sex'].tolist()\n",
    "col4 = df['Age'].tolist()\n",
    "\n",
    "out_df = pd.DataFrame({\n",
    "    'ID': col2,\n",
    "    'Sex': col3,\n",
    "    'Age': col4,\n",
    "    'BC_WC_mean_PRE': col1\n",
    "})\n",
    "\n",
    "out_df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3b7aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('meta/DB_BC_Tempus_2025_09_17_Toni.xlsx', sheet_name='FOLLOW-UP')  # replace with your file path [web:11]\n",
    "col1 = df['BC_WC_mean_FUP'].tolist()\n",
    "\n",
    "out_df = pd.read_csv('data.csv')\n",
    "out_df['BC_WC_mean_FUP'] = col1\n",
    "\n",
    "out_df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68704ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['PRE', 'POST', 'FOLLOW-UP']\n",
    "for label in types:\n",
    "    df = pd.read_excel('meta/DB_BLOB_Tempus_2025_09_17_Toni.xlsx', sheet_name=label)\n",
    "    if label == 'FOLLOW-UP':\n",
    "        label = 'FUP'\n",
    "    col1 = df[f'BLOB_Gluc_{label}'].tolist()\n",
    "    out_df = pd.read_csv('data.csv')\n",
    "    out_df[f'BLOB_Gluc_{label}'] = col1\n",
    "    out_df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064dbe76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efc4fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metabolic_label(df):\n",
    "    \"\"\"\n",
    "    Creates a 'Disease' label based on IDF-like criteria.\n",
    "    Assumes the following columns (adjust names as needed):\n",
    "      - 'Sex' (0 = man, 1 = woman)\n",
    "      - 'Waist_Circum_Mean' (cm)\n",
    "      - 'Triglycerides' (mg/dL)\n",
    "      - 'HDL_Cholesterol' (mg/dL)\n",
    "      - 'Blood_Pressure_Systolic'\n",
    "      - 'Blood_Pressure_Diastolic'\n",
    "      - 'Glucose' (fasting, mg/dL)\n",
    "    \"\"\"\n",
    "\n",
    "    def logic(row):\n",
    "        score = 0\n",
    "\n",
    "        sex = row.get('Sex', 0)  # 0 = man, 1 = woman\n",
    "\n",
    "        # 1. Fasting Glucose > 100 mg/dL\n",
    "        if row.get('Glucose', 0) > 100:\n",
    "            score += 1\n",
    "\n",
    "        # 2. Triglycerides > 150 mg/dL\n",
    "        if row.get('Triglycerides', 0) > 150:\n",
    "            score += 1\n",
    "\n",
    "        # 3. Waist Circumference (central obesity)\n",
    "        waist = row.get('Waist_Circum_mean', 0)\n",
    "        if sex == 0:  # man\n",
    "            if waist > 94:\n",
    "                score += 1\n",
    "        else:  # woman\n",
    "            if waist > 80:\n",
    "                score += 1\n",
    "\n",
    "        # 4. Blood Pressure: systolic >130 or diastolic >85\n",
    "        if (row.get('Blood_Pressure', 0) > 130):\n",
    "            score += 1\n",
    "\n",
    "        # 5. HDL Cholesterol (low)\n",
    "        hdl = row.get('HDL_Cholestrol', 999)\n",
    "        if sex == 0:  # man\n",
    "            if hdl < 40:\n",
    "                score += 1\n",
    "        else:  # woman\n",
    "            if hdl < 50:\n",
    "                score += 1\n",
    "\n",
    "        # Label: 1 if at least 2 abnormal markers\n",
    "        return 1 if score >= 2 else 0\n",
    "\n",
    "    df['Target_Metabolic_Disease'] = df.apply(logic, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6238710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_metabolic_label_clean(df):\n",
    "    \"\"\"\n",
    "    Creates a 'Disease' label based on IDF-like criteria.\n",
    "    Assumes the following columns (adjust names as needed):\n",
    "      - 'Sex' (0 = man, 1 = woman)\n",
    "      - 'Waist_Circum_Mean' (cm)\n",
    "      - 'Triglycerides' (mg/dL)\n",
    "      - 'HDL_Cholesterol' (mg/dL)\n",
    "      - 'Blood_Pressure_Systolic'\n",
    "      - 'Blood_Pressure_Diastolic'\n",
    "      - 'Glucose' (fasting, mg/dL)\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = [\n",
    "        'Sex',\n",
    "        'Waist_Circum_mean',\n",
    "        'Triglycerides',\n",
    "        'HDL_Cholestrol',\n",
    "        'Blood_Pressure',\n",
    "        'Glucose'\n",
    "    ]\n",
    "\n",
    "    def logic(row):\n",
    "        # if ANY required value is missing, skip scoring for this row\n",
    "        if row[required_cols].isna().any():\n",
    "            return np.nan  # or return None, or 0, depending on what \"skip\" should mean for you\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        sex = row.get('Sex', 0)  # 0 = man, 1 = woman\n",
    "\n",
    "        # 1. Fasting Glucose > 100 mg/dL\n",
    "        if row.get('Glucose', 0) > 100:\n",
    "            score += 1\n",
    "\n",
    "        # 2. Triglycerides > 150 mg/dL\n",
    "        if row.get('Triglycerides', 0) > 150:\n",
    "            score += 1\n",
    "\n",
    "        # 3. Waist Circumference (central obesity)\n",
    "        waist = row.get('Waist_Circum_mean', 0)\n",
    "        if sex == 0:  # man\n",
    "            if waist > 94:\n",
    "                score += 1\n",
    "        else:  # woman\n",
    "            if waist > 80:\n",
    "                score += 1\n",
    "\n",
    "        # 4. Blood Pressure: systolic >130 or diastolic >85\n",
    "        if row.get('Blood_Pressure', 0) > 130:\n",
    "            score += 1\n",
    "\n",
    "        # 5. HDL Cholesterol (low)\n",
    "        hdl = row.get('HDL_Cholestrol', 999)\n",
    "        if sex == 0:  # man\n",
    "            if hdl < 40:\n",
    "                score += 1\n",
    "        else:  # woman\n",
    "            if hdl < 50:\n",
    "                score += 1\n",
    "\n",
    "        # Label: 1 if at least 2 abnormal markers\n",
    "        return 1 if score >= 2 else 0\n",
    "\n",
    "    df['Target_Metabolic_Disease'] = df.apply(logic, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a78616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fup.csv')\n",
    "label = create_metabolic_label_clean(df)\n",
    "label.to_csv('label-fup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a74c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pre.csv')\n",
    "label = create_metabolic_label(df)\n",
    "label.to_csv('label-pre.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf37e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa755265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('meta/DB_BC_Tempus_2025_09_17_Toni.xlsx', sheet_name='FOLLOW-UP')\n",
    "col1 = df['BC_WC_mean_FUP'].tolist()\n",
    "col2 = df['ID'].tolist()\n",
    "col3 = df['Sex'].tolist()\n",
    "col4 = df['Age'].tolist()\n",
    "\n",
    "out_df = pd.DataFrame({\n",
    "    'ID': col2,\n",
    "    'Sex': col3,\n",
    "    'Age': col4,\n",
    "    'Waist_Circum_mean': col1\n",
    "})\n",
    "\n",
    "out_df.to_csv('fup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff75ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['BLOB_TG', 'BLOB_HDL', 'BLOB_Gluc', 'BP_SBP_ave']\n",
    "for name in names:\n",
    "    df = pd.read_excel('meta/DB_BLOB_Tempus_2025_09_17_Toni.xlsx', sheet_name='FOLLOW-UP')\n",
    "    col1 = df[f'{name}_FUP'].tolist()\n",
    "    out_df = pd.read_csv('fup.csv')\n",
    "    out_df[f'{name}'] = col1\n",
    "    out_df.to_csv('fup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24363fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
